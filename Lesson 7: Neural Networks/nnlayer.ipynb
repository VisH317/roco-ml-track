{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network From Scratch: Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll work on building a computational neuron and a neural network layer from scratch (Note: we will not implement training/backpropagation from scratch)\n",
    "\n",
    "1. Neuron implementation in vanilla Python - scalar input, scalar output\n",
    "2. Activation functions - how to implement, sigmoid, ReLU\n",
    "3. Neuron layers - properly parallelized neural network layers in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons in Vanilla Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron includes the following:\n",
    "1. A weight - the slope value or multiplication parameter (is a matrix in multidimensional settings)\n",
    "2. A bias - the y-intercept value in the linear equation (is a vector in multidimensional settings)\n",
    "3. An activation function - the nonlinear equation that allows for higher approximation capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuron:\n",
    "    def __init__(self, weight: float, bias: float) -> None:\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, x: float) -> float:\n",
    "        return self.weight * x + self.bias # running the linear transform on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a neuron\n",
    "weight = 2\n",
    "bias = 1\n",
    "\n",
    "neuron = BasicNeuron(weight, bias)\n",
    "\n",
    "neuron(2) # should be 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of activation functions:\n",
    "- ReLU - a function that returns the input value if its positive and 0 if it's negative\n",
    "- Sigmoid - a curve that lies between 0 and 1 in its identity form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# implementing ReLU from scratch\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x) # returns 0 if x < 0\n",
    "\n",
    "# testing relu\n",
    "\n",
    "print(relu(2)) # should be 2\n",
    "print(relu(-2)) # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "4.539786870243442e-05\n",
      "0.9999546021312976\n"
     ]
    }
   ],
   "source": [
    "# Implementing sigmoid from scratch\n",
    "from math import e # euler's number, used for the formula\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + e ** (-x))\n",
    "\n",
    "print(sigmoid(0)) # should be 0.5\n",
    "print(sigmoid(-10)) # should be near zero\n",
    "print(sigmoid(10)) # should be near 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated neuron with activation\n",
    "from typing import Callable # type used for a function passed as an argument\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weight: float, bias: float, activation: Callable) -> None:\n",
    "        self.w = weight\n",
    "        self.b = bias\n",
    "        self.activation = activation\n",
    "    \n",
    "    def __call__(self, x: float):\n",
    "        o = self.w * x + self.b\n",
    "        return self.activation(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with relu\n",
    "\n",
    "neuron_relu = Neuron(1.5, 0.2, relu)\n",
    "neuron_relu(2) # should be 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7211151780228631"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with sigmoid\n",
    "\n",
    "neuron_sig = Neuron(1.5, 0.2, sigmoid)\n",
    "neuron_sig(0.5) # should be â‰ˆ0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other activation functions to try/implement, but we won't get into all of them here\n",
    "\n",
    "Some to look for later:\n",
    "- Tanh - hyperbolic tangent\n",
    "- ELU - exponential linear unit\n",
    "- SiLU - sigmoid linear unit\n",
    "- GLU - gated linear unit (parameterized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized Neuron Layers in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing every individual neuron's output in a layer is tedious and requires lots of extra work from our computer\n",
    "\n",
    "Instead, we can provide the inputs to all the neurons in a layer as a vector. The layer can then output a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defining an activation relu that can work on numpy arrays\n",
    "\n",
    "def parallelized_relu(x: np.array):\n",
    "    return np.vectorize(relu)(x)\n",
    "\n",
    "# our new neuron will look very similar to the original, because numpy can convert regular operations into parallelized vector ones\n",
    "# the main change we need to make is our types\n",
    "\n",
    "class ParallelizedNeuron:\n",
    "    def __init__(self, weight: np.matrix, bias: np.array, activation: Callable) -> None:\n",
    "        self.w = weight\n",
    "        self.b = bias\n",
    "        self.act = activation\n",
    "    \n",
    "    def __call__(self, x: np.array) -> np.array: # here we get and return an array because we get all neuron inputs as a vector\n",
    "        o = np.matmul(self.w, x) + self.b\n",
    "        return self.act(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing our implementation\n",
    "\n",
    "# defining how many neurons of input (the input vector size) and how many neurons in output (output vector size)\n",
    "input_size = 5\n",
    "output_size = 3\n",
    "\n",
    "# generating a weight matrix with random values\n",
    "weight = np.random.rand(output_size, input_size) # this is a projection matrix from the input size to output size, dimensions are input_size x output_size\n",
    "\n",
    "# our bias is added once the vector is transformed from input_size to output_size, it should be of size output_size\n",
    "bias = np.random.rand(output_size)\n",
    "\n",
    "neuron = ParallelizedNeuron(weight, bias, parallelized_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.34583703, 2.11412817, 1.85956705],\n",
       "       [0.81612443, 1.58441558, 1.32985445],\n",
       "       [1.2486747 , 2.01696585, 1.76240472]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing our neuron\n",
    "\n",
    "x = np.random.rand(input_size, 1)\n",
    "\n",
    "neuron(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we use NumPy, we can leverage vectorized operations to compute neural network layers faster\n",
    "\n",
    "We will soon switch to PyTorch, which has the ability to calculate gradients of vectors and matrices, which the neural network will use to learn over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
