{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "1. PyTorch Tensors\n",
    "   1. Calculating PyTorch Gradients\n",
    "2. Building a Simple Neural Network\n",
    "3. Getting our Data\n",
    "4. Setting up our training loop\n",
    "5. Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup/Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use PyTorch to build neural networks because it has support for gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tensor in PyTorch\n",
    "# Tensor: can be a vector, matrix, etc. but can have more dimensions (3, 4, ...)\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1.2, 1.5], [1.6, 1.7]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2000, 1.5000],\n",
       "        [1.6000, 1.7000]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing the tensor's contents: a 2x2 matrix\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.7000]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0.5, 0.7]], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7200, 1.9400]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a dot product!\n",
    "y = x @ tensor\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6600, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's sum the values in the y vector\n",
    "output = torch.sum(y)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a backward pass on output, this computes the gradients\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7000, 3.3000]])\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.7000, 0.7000]])\n"
     ]
    }
   ],
   "source": [
    "# now we can access the gradients of all the computations we did\n",
    "print(x.grad)\n",
    "print(tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these automatically generated gradients by a backward pass, we can use them to shift the weights to optimize for a loss function\n",
    "\n",
    "However, we won't have to access gradients individually like we just did, PyTorch handles a lot of the low-level stuff for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a neural network in NumPy before, but PyTorch has prebuilt primitives to make the process much easier, some important things are:\n",
    "- `torch.nn.Module` - a class we can extend/inherit to create new neural networks\n",
    "- `torch.nn.Linear` - the equivalent of a linear transformation by a weight and a bias, a simple neural network layer\n",
    "- `torch.nn.functional.relu` - the relu activation function\n",
    "- `torch.nn.functional.sigmoid` - the sigmoid activation function\n",
    "\n",
    "PyTorch handles these low-level computations and allows for robust configuration. It will also automatically compute gradients for all of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class NeuralNetwork(nn.Module): # inheriting from the base PyTorch module class\n",
    "    def __init__(self, d_in: int, d_hidden: int, d_out: int) -> None: # the initialization function, which takes 3 parameters\n",
    "        super().__init__() # calls the base constructor for initial setup\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=d_in, out_features=d_hidden) # takes an input vector of size d_in, outputs a vector of size d_hidden\n",
    "        self.linear2 = nn.Linear(in_features=d_hidden, out_features=d_out) # can take input from the previous linear layer to chain together\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor: # this is the method that tells the module what to do with the input\n",
    "        # NOTE: the size of x must be d_in\n",
    "        h = self.linear1(x) # first linear projection\n",
    "        h = nn.functional.relu(h) # relu activation\n",
    "        out = self.linear2(h) # second linear projection\n",
    "        out = nn.functional.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out our model\n",
    "\n",
    "neuralnet = NeuralNetwork(d_in=3, d_hidden=5, d_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4919, -0.0019, -0.4017],\n",
       "        [-0.3067,  0.2290, -0.3321],\n",
       "        [ 0.4997,  0.4455, -0.4952],\n",
       "        [ 0.3109,  0.1463,  0.5115],\n",
       "        [-0.1808,  0.4914,  0.3718]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the weights of our model\n",
    "neuralnet.linear1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3553, -0.1153], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the biases in the second layer of our model\n",
    "neuralnet.linear2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3434, 0.8246, 0.3863])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run a sample input\n",
    "x = torch.rand(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1048], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data to Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data_X, data_y = load_wine(return_X_y=True, as_frame=True)\n",
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing our dataset's length\n",
    "len(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    71\n",
       "0    59\n",
       "2    48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of classes\n",
    "data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SKLearn datasets work well for working with sklearn models, PyTorch datasets (`torch.utils.data.Dataset`) have many utilities that are helpful for training neural networks. Luckily, it is pretty easy to convert between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a PyTorch dataset, we must create a class that inherits from the base class, such as `class WineDataset(torch.utils.data.Dataset):`\n",
    "\n",
    "We must then implement the following methods:\n",
    "- `__init__()` - this initializes the data\n",
    "- `__len__()` - this returns the length of the data\n",
    "- `__getitem__(self, index)` - this returns an item at a specific index given as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WineData(Dataset):\n",
    "    def __init__(self, data_X, data_y):\n",
    "        super().__init__() # initial setup by PyTorch\n",
    "\n",
    "        self.x = data_X\n",
    "        self.y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating these datasets, PyTorch allows us to create DataLoaders (`torch.utils.data.DataLoader`), which can help us retrieve information sequentially or in groups to feed to our model during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While previous methods usually have their training loop under the hood, it's common to write a custom one in PyTorch. We will write a simple loop that consists of looping through the following steps per item in the dataset:\n",
    "\n",
    "1. Retrieve the input features and expected output from our dataloader\n",
    "2. Pass the inputs through the model\n",
    "3. Calculate the loss function given the model's output and the expected output\n",
    "4. Calculate the **gradients** based on the loss function\n",
    "5. *Step* the gradients in the direction that minimizes the loss function\n",
    "   1. We have optimizers that can do this, which PyTorch provides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial setup we'll have to do:\n",
    "- Setting up our loss function\n",
    "- Setting up our optimizer\n",
    "- Setting up storage to hold losses over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of terminology:\n",
    "- Epoch - one full iteration through the dataset, small datasets usually have large numbers of epochs\n",
    "- Batch - a set of data passed to the model at once. Instead of passing one data item each time, passing multiple allows the model to compute them in parallel\n",
    "  - Batch sizes are usually powers of 2 to conform to hardware constraints\n",
    "- Learning rate - determines what percentage of the gradients we alter the weights by\n",
    "  - e.g. if gradient for a parameter is 10 and the learning rate is 0.01, the model will shift that weight by 0.1\n",
    "  - Learning rates must be chosen to balance speed but prevent instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that we can use to organize our data into batches for our model\n",
    "\n",
    "def collate(x: list[tuple[Tensor, Tensor]]):\n",
    "    return torch.stack([i[0] for i in x]), torch.stack([i[1] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training loop!\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# some config\n",
    "NUMBER_OF_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "def train(model: NeuralNetwork, data: WineData):\n",
    "    # setting up our loss function: this is a multiclass classification problem, we'll use cross entropy\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = [] # a list to store our losses over time\n",
    "\n",
    "    # setting up our optimizer, we'll use stochastic gradient descent for this\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(NUMBER_OF_EPOCHS): # for each iteration in the dataset\n",
    "        # we'll set up our data for each epoch\n",
    "\n",
    "        loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) # shuffling prevents the model from learning the dataset's order over time\n",
    "\n",
    "        for index, data_item in tqdm(enumerate(loader), desc=f\"Training our model on epoch {epoch}:\"):\n",
    "            optimizer.zero_grad() # resets our optimizer after the last gradient step\n",
    "            input_data, target = data_item # getting the inputs and outputs from the batch\n",
    "\n",
    "            # compute the outputs of the neural network\n",
    "            output = model(input_data)\n",
    "\n",
    "            #compute the loss\n",
    "            loss = loss_function(output, target) # compares the predicted and expected outputs\n",
    "            loss.backward() # computes gradients automatically\n",
    "\n",
    "            losses.append(loss.item()) # adding our loss to the output\n",
    "\n",
    "            optimizer.step() # alter the weights based on the gradients\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
